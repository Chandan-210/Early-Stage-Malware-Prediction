{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799e7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Bidirectional\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from .useful import *\n",
    "from .Configs import get_all\n",
    "\n",
    "class RecurrentModel():\n",
    "\t\"\"\"Recurrent neural network class with model generator from dictionary of parameters\"\"\"\n",
    "\tdef __init__(self, x_train, y_train,\n",
    "\t\tparameters):\n",
    "\t\tself.current_params = parameters\n",
    "\t\tself.x_train = x_train\n",
    "\t\tself.output_dim = len(y_train[0])\n",
    "\t\tfor key in get_all():\n",
    "\t\t\tif key not in parameters:\n",
    "\t\t\t\traise KeyError(key, \"parameter missing\")\n",
    "\n",
    "\n",
    "\tdef get_model(self):\n",
    "\t\t\"\"\"takes shape of x and y data as tuples, returns model\"\"\"\n",
    "\t\tdims = tuple(list(self.x_train.shape)[1:]) # tuple to list, omit dataset size\n",
    "\t\tmodel = Sequential()\n",
    "\t\t#input\n",
    "\t\treturn_sequences = True\n",
    "\t\tif self.current_params[\"depth\"] == 1:\n",
    "\t\t\treturn_sequences = False\n",
    "\n",
    "\t\tmodel.add(self.__input_layer(dims, return_sequences)) \n",
    "\t\t#hidden\n",
    "\t\tif self.current_params[\"depth\"] > 2:\n",
    "\t\t\tfor i, layer in enumerate(list(range(self.current_params[\"depth\"] - 2))):\n",
    "\t\t\t\tmodel.add(self.__hidden_layer(True))\n",
    "\t\tif self.current_params[\"depth\"] >= 2:\n",
    "\t\t\tmodel.add(self.__hidden_layer(False))\n",
    "\t\t#output\n",
    "\t\tmodel.add(self.__output_layer(self.output_dim)) \n",
    "\n",
    "\t\tif self.current_params[\"optimiser\"] == \"adam\":\n",
    "\t\t\topt = Adam() #use default learning rate for adam\n",
    "\t\telif self.current_params[\"optimiser\"] == \"sgd\":\n",
    "\t\t\topt = SGD(lr=self.current_params[\"learning_rate\"])\n",
    "\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=self.current_params[\"optimiser\"],\n",
    "\t\t\t  loss=self.current_params[\"loss\"],\n",
    "\t\t\t  metrics=['acc']\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\treturn model\n",
    "\n",
    "\tdef __generate_regulariser(self, l1_value, l2_value):\n",
    "\t\t\"\"\" Returns keras l1/l2 regulariser\"\"\"\n",
    "\t\tif l1_value and l2_value:\n",
    "\t\t\treturn l1_l2(l1=l1_value, l2=l2_value)\n",
    "\t\telif l1_value and not l2_value:\n",
    "\t\t\treturn l1(l1_value)\n",
    "\t\telif l2_value:\n",
    "\t\t\treturn l2(l2_value)\n",
    "\t\telse:\n",
    "\t\t\treturn None\n",
    "\n",
    "\n",
    "\tdef __input_layer(self, dims, return_sequences):\n",
    "\t\t\"\"\" Returns GRU or LSTM input layer \"\"\"\t\n",
    "\t\tif self.current_params[\"bidirectional\"] == True:\n",
    "\t\t\treturn Bidirectional(self.__middle_hidden_layer(return_sequences), input_shape=dims)\n",
    "\n",
    "\t\telse:\t\n",
    "\t\t\tif self.current_params[\"layer_type\"]  == \"GRU\":\n",
    "\t\t\t\treturn GRU(self.current_params[\"hidden_neurons\"], \n",
    "\t\t\t\t\tinput_shape=dims,\n",
    "\t\t\t\t\treturn_sequences=return_sequences, \n",
    "\t\t\t\t\tkernel_initializer=self.current_params[\"kernel_initializer\"], \n",
    "\t\t\t\t\trecurrent_initializer=self.current_params[\"recurrent_initializer\"], \n",
    "\t\t\t\t\trecurrent_regularizer=self.__generate_regulariser(self.current_params[\"r_l1_reg\"], self.current_params[\"r_l2_reg\"]), \n",
    "\t\t\t\t\tbias_regularizer=self.__generate_regulariser(self.current_params[\"b_l1_reg\"], self.current_params[\"b_l2_reg\"]),\n",
    "\t\t\t\t\tdropout=self.current_params[\"dropout\"], \n",
    "\t\t\t\t\trecurrent_dropout=self.current_params[\"recurrent_dropout\"]\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\treturn LSTM(self.current_params[\"hidden_neurons\"], \n",
    "\t\t\t\tinput_shape=dims,\n",
    "\t\t\t\treturn_sequences=return_sequences, \n",
    "\t\t\t\tkernel_initializer=self.current_params[\"kernel_initializer\"], \n",
    "\t\t\t\trecurrent_initializer=self.current_params[\"recurrent_initializer\"], \n",
    "\t\t\t\trecurrent_regularizer=self.__generate_regulariser(self.current_params[\"r_l1_reg\"], self.current_params[\"r_l2_reg\"]), \n",
    "\t\t\t\tbias_regularizer=self.__generate_regulariser(self.current_params[\"b_l1_reg\"], self.current_params[\"b_l2_reg\"]),\n",
    "\t\t\t\tdropout=self.current_params[\"dropout\"], \n",
    "\t\t\t\trecurrent_dropout=self.current_params[\"recurrent_dropout\"] \n",
    "\t\t\t)\n",
    "\n",
    "\tdef __hidden_layer(self, return_sequences):\n",
    "\t\t\"\"\" reurns GRU or LSTM hidden layer \"\"\"\n",
    "\t\tlayer = self.__middle_hidden_layer(return_sequences)\n",
    "\n",
    "\t\tif self.current_params[\"bidirectional\"] == True:\n",
    "\t\t\treturn Bidirectional(layer)\n",
    "\t\treturn layer\n",
    "\n",
    "\tdef __middle_hidden_layer(self, return_sequences):\n",
    "\n",
    "\t\tif self.current_params[\"layer_type\"]  == \"GRU\":\n",
    "\t\t\tlayer = GRU(self.current_params[\"hidden_neurons\"], \n",
    "\t\t\t\treturn_sequences=return_sequences, \n",
    "\t\t\t\tkernel_initializer=self.current_params[\"kernel_initializer\"], \n",
    "\t\t\t\trecurrent_initializer=self.current_params[\"recurrent_initializer\"], \n",
    "\t\t\t\trecurrent_regularizer=self.__generate_regulariser(self.current_params[\"r_l1_reg\"], self.current_params[\"r_l2_reg\"]), \n",
    "\t\t\t\tbias_regularizer=self.__generate_regulariser(self.current_params[\"b_l1_reg\"], self.current_params[\"b_l2_reg\"]),\n",
    "\t\t\t\tdropout=self.current_params[\"dropout\"], \n",
    "\t\t\t\trecurrent_dropout=self.current_params[\"recurrent_dropout\"]\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tlayer = LSTM(self.current_params[\"hidden_neurons\"], \n",
    "\t\t\t\treturn_sequences=return_sequences, \n",
    "\t\t\t\tkernel_initializer=self.current_params[\"kernel_initializer\"], \n",
    "\t\t\t\trecurrent_initializer=self.current_params[\"recurrent_initializer\"], \n",
    "\t\t\t\trecurrent_regularizer=self.__generate_regulariser(self.current_params[\"r_l1_reg\"], self.current_params[\"r_l2_reg\"]), \n",
    "\t\t\t\tbias_regularizer=self.__generate_regulariser(self.current_params[\"b_l1_reg\"], self.current_params[\"b_l2_reg\"]),\n",
    "\t\t\t\tdropout=self.current_params[\"dropout\"], \n",
    "\t\t\t\trecurrent_dropout=self.current_params[\"recurrent_dropout\"]\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn layer \n",
    "\n",
    "\tdef __output_layer(self, possible_classes):\n",
    "\t\t\"\"\" Returns output layer of feed-forward neurons \"\"\"\n",
    "\n",
    "\t\treturn Dense(\n",
    "\t\t\tpossible_classes,\n",
    "\t\t\tactivation=self.current_params[\"activation\"],\n",
    "\t\t)\n",
    "\n",
    "class FFNN(RecurrentModel):\n",
    "\tdef __hidden_layer(self, return_sequences):\n",
    "\t\treturn self.__middle_hidden_layer()\n",
    "\n",
    "\tdef __middle_hidden_layer(self):\n",
    "\t\tlayer = Dense(self.current_params[\"hidden_neurons\"], \n",
    "\t\t\tactivation=self.current_params[\"activation\"],\n",
    "\t\t\tkernel_initializer=self.current_params[\"kernel_initializer\"], \n",
    "\t\t\tdropout=self.current_params[\"dropout\"], \n",
    "\t\t)\n",
    "\t\treturn layer \n",
    "\n",
    "\tdef __output_layer(self, possible_classes):\n",
    "\t\t\"\"\" Returns output layer of feed-forward neurons \"\"\"\n",
    "\t\treturn Dense(\n",
    "\t\t\tpossible_classes,\n",
    "\t\t\tactivation=self.current_params[\"activation\"],\n",
    "\t\t)\n",
    "\n",
    "\n",
    "def generate_model(x,y,params,model_type=\"rnn\"):\n",
    "\t\"\"\"return a new model based on \n",
    "\tshape of training data x, shape of training data y \n",
    "\tand parameters\"\"\"\n",
    "\tif model_type == \"rnn\":\n",
    "\t\tmodel_gen = RecurrentModel(x,y,params)\n",
    "\telif model_type ==\"ffnn\":\n",
    "\t\tmodel_gen = FFNN(x,y,params)\n",
    "\telse:\n",
    "\t\traise KeyError(\"model parameter must be rnn or ffnn not {}\".format(model))\n",
    "\treturn model_gen.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

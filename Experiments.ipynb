{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799e7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "import operator\n",
    "from itertools import combinations\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from .RNN import generate_model\n",
    "from .useful import *\n",
    "\n",
    "random.seed(12)\n",
    "np.random.seed(12)\n",
    "\n",
    "class Experiment():\n",
    "\t\"\"\"docstring for Experiment\"\"\"\n",
    "\tdef __init__(self, parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=None, y_test=None,\n",
    "\t\tx_train=None, y_train=None,\n",
    "\t\tdata=None, folds=10,\n",
    "\t\tfolder_name=str(time.time()),\n",
    "\t\tthresholding=False, threshold=0.5, model_type=\"rnn\"):\n",
    "\n",
    "\t\tassert (type(folds) is int) or (data == None), \"folds must be an integer if data tuple is provided\"\n",
    "\t\tassert (search_algorithm.lower() == \"grid\") or (search_algorithm.lower() == \"random\"), \"Only 'grid' and 'random' permissible values for search_algorithm\"\n",
    "\n",
    "\t\tself.headers = [\n",
    "\t\t\t'bytes.common_bytes1',\n",
    "            'bytes.common_bytes2',\n",
    "            'bytes.common_bytes3',\n",
    "            'bytes.common_bytes4',\n",
    "            'bytes.common_bytes5',\n",
    "            'bytes.common_bytes6',\n",
    "            'bytes.entropy',\n",
    "            'bytes.longest_sequence.byte',\n",
    "            'bytes.longest_sequence.length',\n",
    "            'bytes.longest_sequence.offset',\n",
    "            'bytes.max_entropy',\n",
    "            'bytes.min_entropy',\n",
    "            'bytes.null_bytes',\n",
    "            'bytes.printable',\n",
    "            'bytes.rarest_bytes1',\n",
    "            'bytes.rarest_bytes2',\n",
    "            'bytes.rarest_bytes3',\n",
    "            'bytes.rarest_bytes4',\n",
    "            'bytes.rarest_bytes5',\n",
    "            'bytes.rarest_bytes6',\n",
    "            'bytes.unique_bytes',\n",
    "            'bytes.white_spaces',\n",
    "            'elf.e_phentsize',\n",
    "            'elf.e_phnum',\n",
    "            'elf.e_phoff',\n",
    "            'elf.e_shentsize',\n",
    "            'elf.e_shnum',\n",
    "            'elf.e_shoff',\n",
    "            'elf.e_shstrndx',\n",
    "            'elf.nsections',\n",
    "            'elf.nsegments',\n",
    "            'elf.stripped',\n",
    "            'elf.stripped_sections',\n",
    "            'funcover.findcrypt.total',\n",
    "            'funcover.idapro.average_bytes_func',\n",
    "            'funcover.idapro.avg_basic_blocks',\n",
    "            'funcover.idapro.avg_cyclomatic_complexity',\n",
    "            'funcover.idapro.avg_loc',\n",
    "            'funcover.idapro.badstack',\n",
    "            'funcover.idapro.branch_instr',\n",
    "            'funcover.idapro.bytes_func',\n",
    "            'funcover.idapro.call_instr',\n",
    "            'funcover.idapro.func_loc',\n",
    "            'funcover.idapro.indirect_branch_instr',\n",
    "            'funcover.idapro.indirect_call_instr',\n",
    "            'funcover.idapro.loc',\n",
    "            'funcover.idapro.max_basic_blocks',\n",
    "            'funcover.idapro.max_cyclomatic_complexity',\n",
    "            'funcover.idapro.nfuncs',\n",
    "            'funcover.idapro.overlapped_instr',\n",
    "            'funcover.idapro.percent_load_covered',\n",
    "            'funcover.idapro.percent_text_covered',\n",
    "            'funcover.idapro.syscall_instr',\n",
    "            'libide.lstrings.libc.matches',\n",
    "            'unpacker.packed']\n",
    "\n",
    "\t\t#hyperparameters\n",
    "\t\tself.original_h_params = parameters\n",
    "\t\tself.h_params = parameters\n",
    "\n",
    "\t\t# set up parameter search space depending on algorithm\n",
    "\t\tself.search_algorithm = search_algorithm\n",
    "\t\tself.current_params = {}\n",
    "\n",
    "\t\tif self.search_algorithm == \"grid\":\n",
    "\t\t\tself.h_params = dict([(key, list(self.h_params[key])) for key in self.h_params])\n",
    "\t\t\tself.original_h_params = deepcopy(self.h_params)\n",
    "\t\t\tself.current_params = dict([(key, self.h_params[key][0]) for key in self.h_params])\n",
    "\t\tif self.search_algorithm == \"random\":\n",
    "\t\t\tself.__list_to_dict_params() # list instances to even probability dictionary instances\n",
    "\t\t\tself.__map_to_0_1() # re-adjust any instances in which sum(probabilities) > 1\n",
    "\n",
    "\t\t# metrics and writer objects, to be assigned when first experiment written up\n",
    "\t\tself.folder_name = check_filename(folder_name)\n",
    "\t\tself.experiment_id = 0\n",
    "\t\tself.metrics_headers = None\n",
    "\t\tself.metrics_writer = None\n",
    "\n",
    "\t\t#Model type\n",
    "\t\tself.model_type = model_type\n",
    "\n",
    "\t\t# Thresholding set up\n",
    "\t\tself.thresholding = thresholding\n",
    "\t\tif self.thresholding:\n",
    "\t\t\tself.min_threshold = threshold + K.epsilon()\n",
    "\t\t\tself.temp_min_threshold = threshold + K.epsilon()\n",
    "\n",
    "\t\t# test-train experiment - on all dataset\n",
    "\t\tif (data == None) or (folds == None):\n",
    "\t\t\tself.folds = None\n",
    "\t\t\tself.X_TRAIN = x_train\n",
    "\t\t\tself.Y_TRAIN = y_train\n",
    "\t\t\tself.X_TEST = x_test\n",
    "\t\t\tself.Y_TEST = y_test\n",
    "\t\t\tprint(\"Test-train experiment\")\n",
    "\t\t# k-fold cross-validation experiment - just on training set\n",
    "\t\telse:\n",
    "\t\t\tassert folds != None, \"Supply number of folds for k-fold cross validation or supply x_train, y_train, x_test, y_test\"\n",
    "\t\t\tself.folds = folds\n",
    "\t\t\tself.x = data[0]\n",
    "\t\t\tself.y = data[1]\n",
    "\t\t\tprint(self.folds, \"- fold cross validation experiment\")\n",
    "\n",
    "\n",
    "\n",
    "\tdef __list_to_dict_params(self):\n",
    "\t\t\tfor key in self.h_params:\n",
    "\t\t\t\tif type(self.h_params[key]) is list:\n",
    "\t\t\t\t\tself.h_params[key] = dict([(x, 1/(len(self.h_params[key]) + K.epsilon() )) for x in self.h_params[key]])\n",
    "\n",
    "\tdef __map_to_0_1(self):\n",
    "\t\t\"\"\"maps input probabilities to values between 0 and 1, preserving scalar relationships\"\"\"\n",
    "\t\tfor key in self.h_params:\n",
    "\t\t\trunning_total = 0\n",
    "\t\t\tscalar = 1/(sum(self.h_params[key].values()))\n",
    "\t\t\tfor possible_value in self.h_params[key]:\n",
    "\t\t\t\tif self.h_params[key][possible_value] < 0:\n",
    "\t\t\t\t\traise ValueError(\"Negative hyperparameter probabilities are not allowed ({} for {})\").format(self.h_params[key][possible_value], possible_value)\n",
    "\t\t\t\tnew_value = self.h_params[key][possible_value] * scalar\n",
    "\t\t\t\tself.h_params[key][possible_value] = new_value + running_total\n",
    "\t\t\t\trunning_total += new_value\n",
    "\n",
    "\tdef __random_config(self):\n",
    "\t\t\"\"\"randomly generate a configuration of hyperparameters from dictionary self.h_params\"\"\"\n",
    "\t\tfor key in self.h_params:\n",
    "\t\t\tchoice = random.random()\n",
    "\t\t\tsorted_options = sorted(self.h_params[key].items(), key=operator.itemgetter(1))\n",
    "\t\t\tfor option in sorted_options:\n",
    "\t\t\t\tif choice < option[1]:\n",
    "\t\t\t\t\tself.current_params[key] = option[0]\n",
    "\t\t\t\t\tbreak\n",
    "\t\tif self.current_params[\"optimiser\"] == \"adam\":\n",
    "\t\t\tself.current_params[\"learning_rate\"] = 0.001\n",
    "\t\tprint()\n",
    "\n",
    "\tdef run_one_experiment(self):\n",
    "\t\tprint(\"run one experiment - orig\")\n",
    "\t\t#Get new configuration if random search\n",
    "\t\tif self.search_algorithm == \"random\":\n",
    "\t\t\tself.__random_config()\n",
    "\n",
    "\t\tself.experiment_id += 1\n",
    "\t\tprint(\"running expt\", self.experiment_id, \"of\", self.num_experiments)\n",
    "\t\tprint(self.current_params)\n",
    "\n",
    "\t\tself.metrics = {}\n",
    "\t\tself.current_fold = 1\n",
    "\t\tself.accuracy_scores = []\n",
    "\n",
    "\t\t# k-fold cross-validation\n",
    "\t\tif self.folds != None:\n",
    "\t\t\ty = deepcopy(self.y)\n",
    "\t\t\tx = deepcopy(self.x)\n",
    "\n",
    "\t\t\t#remove short seqeunces and store indicies of kept items\n",
    "\t\t\tx, y, identifiers = remove_short_idx(x, y, list(range(len(y))), self.current_params[\"sequence_length\"])\n",
    "\t\t\tlabels = {}\n",
    "\t\t\ttemp_y = deepcopy(y).flatten() # get labels as flat array to find stratified folds\n",
    "\n",
    "\t\t\tfor i, class_label in zip(identifiers, temp_y):\n",
    "\t\t\t\tif class_label in labels:\n",
    "\t\t\t\t\tlabels[class_label].append(i)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlabels[class_label] = [i]\n",
    "\n",
    "\t\t\t#split into number of folds\n",
    "\t\t\tfold_indicies = [[] for x in range(self.folds)]\n",
    "\t\t\t# Want to represent class distribution in each set (stratified split)\n",
    "\t\t\t# Divide indicies list in chunks of size folds\n",
    "\t\t\tfor key in labels:\n",
    "\t\t\t\tlabels[key] = to_chunks(labels[key], self.folds)\n",
    "\t\t\t\tfor i, fold_ids in enumerate(labels[key]):\n",
    "\t\t\t\t\tfold_indicies[i] += fold_ids\n",
    "\n",
    "\t\t\tfold_indicies = np.array([[int(x) for x in index_set] for index_set in fold_indicies])\n",
    "\t\t\t#take new copies of the original to use indicies from the original sets\n",
    "\t\t\tx, y = deepcopy(self.x), deepcopy(self.y)\n",
    "\n",
    "\t\t\tfor i in list(range(self.folds)):\n",
    "\t\t\t\ttest = np.array([(i) % self.folds]) # one fold is test set\n",
    "\t\t\t\ttrain = np.array([i for i in range(self.folds) if i not in [test]]) # remaining folds are training set ( not in [test, val])\n",
    "\n",
    "\t\t\t\ttest_idxs = np.concatenate(tuple([fold_indicies[i] for i in test]))\n",
    "\t\t\t\ttrain_idxs = np.concatenate(tuple([fold_indicies[i] for i in train]))\n",
    "\n",
    "\t\t\t\tself.x_train, self.y_train = truncate_and_tensor(x[train_idxs], y[train_idxs], self.current_params[\"sequence_length\"])\n",
    "\t\t\t\tself.x_test, self.y_test = truncate_and_tensor(x[test_idxs], y[test_idxs], self.current_params[\"sequence_length\"])\n",
    "\n",
    "\t\t\t\tself.test_idxs = test_idxs\n",
    "\n",
    "\t\t\t\tstop = self.set_up_model()\n",
    "\t\t\t\tif stop:\n",
    "\t\t\t\t\treturn\n",
    "\n",
    "\t\t\t\tself.current_fold += 1\n",
    "\n",
    "\t\t# test-train\n",
    "\t\telse:\n",
    "\t\t\tself.x_train = deepcopy(self.X_TRAIN)\n",
    "\t\t\tself.y_train = deepcopy(self.Y_TRAIN)\n",
    "\t\t\tself.x_test = deepcopy(self.X_TEST)\n",
    "\t\t\tself.y_test = deepcopy(self.Y_TEST)\n",
    "\n",
    "\t\t\tself.test_idxs = np.array(range(1, len(self.y_test) + 1)) / 10 #divide by 10 to distinguish from training/10-fold data\n",
    "\n",
    "\t\t\t# remove short sequences - store indicies for test data\n",
    "\t\t\tself.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params[\"sequence_length\"])\n",
    "\t\t\tself.x_test, self.y_test, self.test_idxs = remove_short_idx(self.x_test, self.y_test, self.test_idxs, self.current_params[\"sequence_length\"])\n",
    "\n",
    "\t\t\tself.set_up_model()\n",
    "\n",
    "\n",
    "\tdef set_up_model(self):\n",
    "\t\t# Leave out feature if specified in dictionary\n",
    "\t\tif \"leave_out_feature\" in self.current_params:\n",
    "\t\t\tprint(\"Omitting feature:\", self.headers[self.current_params[\"leave_out_feature\"]])\n",
    "\t\t\tself.x_train = np.delete(self.x_train, self.current_params[\"leave_out_feature\"], 2)\n",
    "\t\t\tself.x_test = np.delete(self.x_test, self.current_params[\"leave_out_feature\"], 2)\n",
    "\n",
    "\t\t#Shuffle data\n",
    "\t\tself.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])\n",
    "\t\tself.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])\n",
    "\n",
    "\t\t#scale data by test data mean and variance\n",
    "\t\tmeans, stdvs = get_mean_and_stdv(self.x_train)\n",
    "\t\tself.x_train = scale_array(self.x_train, means, stdvs)\n",
    "\t\tself.x_test = scale_array(self.x_test, means, stdvs)\n",
    "\n",
    "\t\t#Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tprint(\"train, test set size (x):\", self.x_train.shape, self.x_test.shape)\n",
    "\t\tmodel = generate_model(self.x_train, self.y_train, self.current_params, model_type=self.model_type)\n",
    "\n",
    "\t\t#if self.current_fold == 1:\n",
    "\t\t#\tprint(model.summary())\n",
    "\n",
    "\t\treturn self.train_model(model) #Returns TRUE if accuracy below threshold\n",
    "\n",
    "\n",
    "\tdef train_model(self, model):\n",
    "\t\t\"\"\"run one fold and write up results\"\"\"\n",
    "\t\tprint(\"\t\tfold \", self.current_fold, \"of\", self.folds)\n",
    "\t\tmetrics = self.metrics\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\t\tearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=8, verbose=0, mode='auto')\n",
    "\n",
    "\t\tstart_train = time.time()\n",
    "\t\th = model.fit(\n",
    "\t\t\tself.x_train, self.y_train,\n",
    "\t\t\tbatch_size=self.current_params[\"batch_size\"],\n",
    "\t\t\tepochs=self.current_params[\"epochs\"],\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tverbose=0,\n",
    "\t\t\tcallbacks=[reset_states])\n",
    "\n",
    "\t\tend_train = time.time()\n",
    "\n",
    "\t\tmetrics[\"train_acc\"] = h.history[\"acc\"]\n",
    "\n",
    "\t\tstart_test = time.time()\n",
    "\t\tpred_Y = model.predict(self.x_test, batch_size=self.current_params[\"batch_size\"])\n",
    "\t\tmetrics[\"preds\"] = [x[0] for x in pred_Y]\n",
    "\t\tend_test = time.time()\n",
    "\t\tmetrics[\"truth\"] = self.y_test.flatten().tolist()\n",
    "\t\tmetrics[\"categorical_preds\"] = [np.round(x) for x in metrics[\"preds\"]]\n",
    "\t\tmetrics[\"fscore\"] = f1_score(metrics[\"truth\"], metrics[\"categorical_preds\"])\n",
    "\t\tmetrics[\"accuracy\"] = accuracy_score(metrics[\"truth\"], metrics[\"categorical_preds\"])\n",
    "\t\tmetrics[\"experiment_id\"] = self.experiment_id\n",
    "\t\tmetrics[\"training_size\"] = len(self.y_train)\n",
    "\t\tmetrics[\"test_size\"] = len(self.y_test)\n",
    "\t\tmetrics[\"train_time\"] = end_train - start_train\n",
    "\t\tmetrics[\"test_time\"] = end_test - start_test\n",
    "\t\tmetrics[\"fold_id\"] = self.current_fold\n",
    "\t\tmetrics[\"test_idxs\"] = self.test_idxs\n",
    "\n",
    "\t\ttn, fp, fn, tp =  confusion_matrix(metrics[\"truth\"], metrics[\"categorical_preds\"]).ravel()\n",
    "\t\tself.metrics[\"tp\"] = tp/metrics[\"truth\"].count(1)\n",
    "\t\tself.metrics[\"tn\"] = tn/metrics[\"truth\"].count(0)\n",
    "\t\tself.metrics[\"fp\"] = fp/metrics[\"truth\"].count(0)\n",
    "\t\tself.metrics[\"fn\"] = fn/metrics[\"truth\"].count(1)\n",
    "\n",
    "\t\tif not self.metrics_headers:\n",
    "\t\t\t#Create files and Write file headers\n",
    "\t\t\tos.mkdir(self.folder_name)\n",
    "\t\t\tself.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())\n",
    "\t\t\tself.metrics_file = open(\"{}/results.csv\".format(self.folder_name), \"w\")\n",
    "\t\t\tself.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)\n",
    "\t\t\tself.metrics_writer.writeheader()\n",
    "\n",
    "\t\t#Write up metric results\n",
    "\n",
    "\t\tself.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))\n",
    "\n",
    "\t\t#Search type changes\n",
    "\t\tprint(\"acc:\", metrics[\"accuracy\"], \"fscore:\", metrics[\"fscore\"])\n",
    "\t\tfor x in ['tn', 'fp', 'fn', 'tp']:\n",
    "\t\t\tprint(\"{}: {}\".format(x, self.metrics[x]), end=\" \")\n",
    "\t\tprint()\n",
    "\t\t\"\"\",\n",
    "\t\t\t\"mal%:\", np.round(\n",
    "\t\t\t\tmetrics[\"truth\"].count(1)/len(metrics[\"truth\"]), decimals=2)\n",
    "\t\t\t,\"tp\", tp/metrics[\"truth\"].count(1), \"tn\", tn/metrics[\"truth\"].count(0)\n",
    "\t\t\t)\n",
    "\t\t\"\"\"\n",
    "\t\t#make space in memory\n",
    "\t\tdel model\n",
    "\t\tgc.collect()\n",
    "\n",
    "\t\tself.accuracy_scores.append(metrics[\"accuracy\"])\n",
    "\t\tif self.current_fold == self.folds:\n",
    "\t\t\taverage_acc = sum(self.accuracy_scores) / len(self.accuracy_scores)\n",
    "\t\t\tprint(\"average acc:\", average_acc)\n",
    "\n",
    "\t\tif self.thresholding:\n",
    "\t\t\tif metrics[\"accuracy\"] < self.temp_min_threshold:\n",
    "\t\t\t\treturn True\n",
    "\t\t\t#On last fold check if average accuracy > current threshold, update temporary minimum to smallest from folds accuracy\n",
    "\t\t\telif (self.current_fold == self.folds) and (average_acc > self.min_threshold):\n",
    "\t\t\t\tself.temp_min_threshold = min(self.accuracy_scores)\n",
    "\t\t\t\tself.min_threshold = average_acc\n",
    "\t\t\t\tprint(\"* * * NEW RECORD avg acc:\", average_acc, \"min acc:\", self.temp_min_threshold)\n",
    "\n",
    "\t\treturn False # Only return true to stop models running\n",
    "\n",
    "\n",
    "\tdef run_experiments(self, num_experiments=100):\n",
    "\t\t# GRID SEARCH\n",
    "\t\t#Find total possible configurations from options\n",
    "\t\tself.total = 1\n",
    "\t\tfor key in self.original_h_params:\n",
    "\t\t\tself.total *= len(self.original_h_params[key])\n",
    "\n",
    "\t\tif self.search_algorithm == \"grid\":\n",
    "\t\t\theader_list = list(self.h_params.keys()) #Fixed keys list to loop in order\n",
    "\t\t\tcountdown = len(self.h_params) - 1\n",
    "\t\t\tself.num_experiments = self.total\n",
    "\t\t\tprint(\"grid search of \", self.total, \"configurations...\")\n",
    "\t\t\tself.loop_values(header_list, countdown)\n",
    "\n",
    "\t\t# RANDOM SEARCH\n",
    "\t\telif self.search_algorithm == \"random\":\n",
    "\t\t\tself.num_experiments = num_experiments\n",
    "\t\t\tprint(\"random search of \", self.num_experiments, \"configurations of a possible\", self.total, \"configurations\")\n",
    "\t\t\twhile(self.experiment_id <= self.num_experiments):\n",
    "\t\t\t\tself.run_one_experiment()\n",
    "\n",
    "\n",
    "\t\t# Experiments run - close data files\n",
    "\t\tprint(self.experiment_id, \" models run.\")\n",
    "\n",
    "\t\tself.metrics_file.close()\n",
    "\n",
    "\n",
    "\tdef loop_values(self, header_list, countdown):\n",
    "\t\t# loop through all possible configurations in original parameter dictionary\n",
    "\t\t# http://stackoverflow.com/questions/7186518/function-with-varying-number-of-for-loops-python\n",
    "\t\tif (countdown > 0):\n",
    "\t\t\tfor i in self.original_h_params[header_list[countdown]]:\n",
    "\t\t\t\tself.current_params[header_list[countdown]] = i\n",
    "\t\t\t\tself.loop_values(header_list, countdown - 1)\n",
    "\t\telse:\n",
    "\t\t\tfor i in self.original_h_params[header_list[countdown]]:\n",
    "\t\t\t\tself.current_params[header_list[countdown]] = i\n",
    "\t\t\t\tself.run_one_experiment()\n",
    "\n",
    "\n",
    "\n",
    "class Increase_Snaphot_Experiment(Experiment):\n",
    "\t\"\"\"Experiment to look at change in data snapshot intervals\"\"\"\n",
    "\tdef __init__(self, parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=None, y_test=None,\n",
    "\t\tx_train=None, y_train=None,\n",
    "\t\tdata=None, folds=10,\n",
    "\t\tfolder_name=str(time.time()),\n",
    "\t\tthresholding=False, threshold=0.5, run_on_factors=True, model_type=\"rnn\"):\n",
    "\n",
    "\t\tsuper(Increase_Snaphot_Experiment, self).__init__(parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=x_test, y_test=y_test,\n",
    "\t\tx_train=x_train, y_train=y_train,\n",
    "\t\tdata=data, folds=folds,\n",
    "\t\tfolder_name=folder_name,\n",
    "\t\tthresholding=thresholding, threshold=threshold)\n",
    "\n",
    "\t\tself.run_on_factors = run_on_factors\n",
    "\n",
    "\tdef set_up_model(self):\n",
    "\t\t# Keep only every step-th data snapshot, do not run unless new data involved\n",
    "\t\tif (self.run_on_factors and ((self.current_params[\"sequence_length\"] - 1) % self.current_params[\"step\"] == 0)) or (self.run_on_factors == False):\n",
    "\t\t\tself.x_train = self.x_train[:,::self.current_params[\"step\"]]\n",
    "\t\t\tself.x_test = self.x_test[:,::self.current_params[\"step\"]]\n",
    "\n",
    "\t\t\t# Shuffle data\n",
    "\t\t\tself.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])\n",
    "\t\t\tself.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])\n",
    "\n",
    "\t\t\t# scale data by test data mean and variance\n",
    "\t\t\tmeans, stdvs = get_mean_and_stdv(self.x_train)\n",
    "\t\t\tself.x_train = scale_array(self.x_train, means, stdvs)\n",
    "\t\t\tself.x_test = scale_array(self.x_test, means, stdvs)\n",
    "\n",
    "\t\t\t# Output size - in future delete any cols for categorical which are all zero\n",
    "\t\t\tmodel = generate_model(self.x_train, self.y_train,\n",
    "\t\t\t\tself.current_params, model_type=self.model_type)\n",
    "\n",
    "\t\t\tif self.current_fold == 1:\n",
    "\t\t\t\tprint(model.summary())\n",
    "\n",
    "\t\t\t#UNINDENT line BELOW\n",
    "\t\t\treturn self.train_model(model) #Returns TRUE if accuracy below threshold\n",
    "\n",
    "\n",
    "class Ensemble(Experiment):\n",
    "\tdef write_up_models(self, models, preds):\n",
    "\t\tself.metrics[\"truth\"] = self.y_test.flatten()\n",
    "\t\tself.metrics[\"preds\"] = np.array(preds).max(axis=0)\n",
    "\t\tself.metrics[\"categorical_preds\"] = [np.round(x) for x in self.metrics[\"preds\"]]\n",
    "\t\tself.metrics[\"fscore\"] = f1_score(self.metrics[\"truth\"], self.metrics[\"categorical_preds\"])\n",
    "\t\tself.metrics[\"accuracy\"] = accuracy_score(self.metrics[\"truth\"], self.metrics[\"categorical_preds\"])\n",
    "\n",
    "\t\twriteable = merge_two_dicts(self.current_params, self.metrics)\n",
    "\n",
    "\t\tif not self.metrics_headers:\n",
    "\t\t\t#Create files and Write file headers\n",
    "\t\t\tos.mkdir(self.folder_name)\n",
    "\t\t\tself.metrics_headers = writeable.keys()\n",
    "\t\t\tself.metrics_file = open(\"{}/results.csv\".format(self.folder_name), \"w\")\n",
    "\t\t\tself.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)\n",
    "\t\t\tself.metrics_writer.writeheader()\n",
    "\t\tself.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))\n",
    "\n",
    "\t\t#Search type changes\n",
    "\t\tprint(\"acc:\", self.metrics[\"accuracy\"])\n",
    "\n",
    "\t\t#make space in memory\n",
    "\t\tfor model in models:\n",
    "\t\t\tdel model\n",
    "\t\tgc.collect()\n",
    "\n",
    "\n",
    "class  Ensemble_configurations(Ensemble):\n",
    "\t\"\"\"Average the predictions of multiple models passed as a list as configurations\"\"\"\n",
    "\t#setup this class adding our own custom member variables\n",
    "\tdef __init__(self, parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=None, y_test=None,\n",
    "\t\tx_train=None, y_train=None,\n",
    "\t\tdata=None, folds=10,\n",
    "\t\tfolder_name=str(time.time()),\n",
    "\t\tthresholding=False, threshold=0.5, run_on_factors=True,\n",
    "\t\tmodel_type=\"rnn\", batch_size=64):\n",
    "\n",
    "\t\t#baseclass-Experiment, Ensemble-childclass of Experiment, Ensemble_confg-child of Ensemble\n",
    "\t\t#setup baseclass- Experiment with variable from setting up Ensemble_configurations class\n",
    "\t\tsuper(Ensemble_configurations, self).__init__(parameters[0],\n",
    "\t\t\tsearch_algorithm=\"grid\",\n",
    "\t\t\tx_test=x_test, y_test=y_test, x_train=x_train, y_train=y_train,\n",
    "\t\t\tdata=data, folds=folds, folder_name=folder_name,\n",
    "\t\t\tthresholding=thresholding, threshold=threshold, model_type=model_type)\n",
    "\t\tself.search_algorithm = None\n",
    "\n",
    "\t\t# Sequence length is the only shared variable - get distinct values\n",
    "\t\tself.sequence_lengths = set([s for seq_lens in [config[\"sequence_length\"] for config in parameters] for s in seq_lens])\n",
    "\n",
    "\t\t#Only one parameter considered from each config, not a search experiment\n",
    "\t\tif not(all([all([len(config[key]) == 1 for key in config]) for config in parameters])):\n",
    "\t\t\t\"not a search experiment, only one parameter will be used from each configuration\"\n",
    "\n",
    "\t\t# dictionary values to list\n",
    "\t\tself.configurations = []\n",
    "\t\tfor config in parameters:\n",
    "\t\t\tconfig = dict([(key, list(config[key])[0]) for key in config])\n",
    "\t\t\tself.configurations.append(config)\n",
    "\t\t\t#Can set all batch sizes the same if specified\n",
    "\t\t\tif batch_size:\n",
    "\t\t\t\tconfig[\"batch_size\"] = batch_size\n",
    "\n",
    "\n",
    "\tdef set_up_model(self):\n",
    "\t\t# Shuffle data\n",
    "\t\t#self.x_train, self.y_train, self.x_val, self.y_val = extract_val_set_binary(self.x_train, self.y_train, 0.1)\n",
    "\t\tself.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])\n",
    "\t\tself.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])\n",
    "\t\t#self.x_val, self.y_val = unison_shuffled_copies([self.x_val, self.y_val])\n",
    "\n",
    "\t\t# scale data by test data mean and variance\n",
    "\t\tmeans, stdvs = get_mean_and_stdv(self.x_train)\n",
    "\t\tself.x_train = scale_array(self.x_train, means, stdvs)\n",
    "\t\tself.x_test = scale_array(self.x_test, means, stdvs)\n",
    "\t\t#self.x_val = scale_array(self.x_val, means, stdvs)\n",
    "\n",
    "\t\t# Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tmodels = [generate_model(\n",
    "\t\t\tself.x_train, self.y_train, config, model_type=self.model_type\n",
    "\t\t\t) for config in self.configurations]\n",
    "\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\t\tpreds = []\n",
    "\t\t#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=, verbose=0, mode='auto')\n",
    "\n",
    "\t\tprint(\"train, test set size (x):\", self.x_train.shape, self.x_test.shape)\n",
    "\t\tprint(\"train mal:\",self.y_train.flatten().tolist().count(1) , \"ben:\",self.y_train.flatten().tolist().count(0) , \"test mal:\", self.y_test.flatten().tolist().count(1), \"ben:\", self.y_test.flatten().tolist().count(0))\n",
    "\t\tfor model, config in zip(models, self.configurations):\n",
    "\t\t\th = model.fit(\n",
    "\t\t\t\tself.x_train, self.y_train,\n",
    "\t\t\t\tbatch_size=config[\"batch_size\"],\n",
    "\t\t\t\tepochs=config[\"epochs\"],\n",
    "\t\t\t\tshuffle=True,\n",
    "\t\t\t\tverbose=0,\n",
    "\t\t\t\tcallbacks=[reset_states]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\td = config[\"description\"]\n",
    "\n",
    "\t\t\tself.metrics[\"train_acc_{}\".format(d)] = h.history[\"acc\".format(d)]\n",
    "\t\t\tpred_Y = model.predict(self.x_test, batch_size=config[\"batch_size\"])\n",
    "\t\t\tpreds.append(pred_Y)\n",
    "\t\t\t#self.metrics[\"val_preds_{}\"] = model.predict(self.x_val, batch_size=config[\"batch_size\"]).flatten().tolist()\n",
    "\t\t\t#self.metrics[\"val_truth\"] = self.y_val.flatten().tolist()\n",
    "\t\t\tself.metrics[\"test_idxs\"] = self.test_idxs\n",
    "\t\t\tself.metrics[\"preds_{}\".format(d)] = pred_Y.flatten()\n",
    "\n",
    "\t\t\tself.metrics[\"acc_{}\".format(d)] = accuracy_score(\n",
    "\t\t\t\tself.y_test.flatten().round(),\n",
    "\t\t\t\tself.metrics[\"preds_{}\".format(d)].round()\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tprint(d, \"acc :\", self.metrics[\"acc_{}\".format(d)], \"f1\", f1_score(self.y_test.flatten().round(),\n",
    "\t\t\t\tself.metrics[\"preds_{}\".format(d)].round()))\n",
    "\n",
    "\t\tself.write_up_models(models, preds)\n",
    "\t\treturn False\n",
    "\n",
    "\tdef run_experiments(self):\n",
    "\t\t# Only changeable parameter is sequence length\n",
    "\t\tself.num_experiments = len(self.sequence_lengths)\n",
    "\n",
    "\t\tfor s in self.sequence_lengths:\n",
    "\t\t\tself.current_params = {\"sequence_length\": s}\n",
    "\t\t\tself.run_one_experiment()\n",
    "\n",
    "\t\t# Experiments run - close data files\n",
    "\t\tprint(self.experiment_id, \" models run.\")\n",
    "\n",
    "\t\tself.metrics_file.close()\n",
    "\n",
    "\n",
    "class Ensemble_sub_sequences(Ensemble):\n",
    "\t\"\"\"Ensemble models for sub-seqeunces of data\"\"\"\n",
    "\tdef __init__(self, parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=None, y_test=None,\n",
    "\t\tx_train=None, y_train=None,\n",
    "\t\tdata=None, folds=10,\n",
    "\t\tfolder_name=str(time.time()),\n",
    "\t\tthresholding=False, threshold=0.5):\n",
    "\n",
    "\t\tsuper(Ensemble_sub_sequences, self).__init__(parameters, search_algorithm=\"grid\", x_test=x_test, y_test=y_test, x_train=x_train, y_train=y_train, data=data, folds=folds, folder_name=folder_name, thresholding=thresholding, threshold=threshold)\n",
    "\t\tself.search_algorithm = None\n",
    "\n",
    "\t\t# Sequence length is the only shared variable - get distinct values\n",
    "\t\tself.sequence_lengths = self.h_params[\"sequence_length\"]\n",
    "\n",
    "\tdef set_up_model(self):\n",
    "\t\t# Shuffle data\n",
    "\t\tself.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])\n",
    "\t\tself.x_test, self.y_test, self.test_idxs = unison_shuffled_copies([self.x_test, self.y_test, self.test_idxs])\n",
    "\n",
    "\t\t# scale data by test data mean and variance\n",
    "\t\tmeans, stdvs = get_mean_and_stdv(self.x_train)\n",
    "\t\tself.x_train = scale_array(self.x_train, means, stdvs)\n",
    "\t\tself.x_test = scale_array(self.x_test, means, stdvs)\n",
    "\n",
    "\t\t# Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tmodels = []\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\t\tmetrics = self.metrics\n",
    "\t\tpreds = []\n",
    "\n",
    "\t\ttraining_sets = []\n",
    "\t\ttesting_sets = []\n",
    "\t\tidx_len_tuples = []\n",
    "\n",
    "\t\t#Create multiple training sets\n",
    "\t\tfor length in list(range(1, self.current_params[\"sequence_length\"])):\n",
    "\t\t\tmini_train_X, b = into_sliding_chunk_arrays(self.x_train, length)\n",
    "\t\t\tmini_test_X, b = into_sliding_chunk_arrays(self.x_test, length)\n",
    "\t\t\tidx_len_tuples += b\n",
    "\t\t\ttraining_sets += mini_train_X #append for merge2\n",
    "\t\t\ttesting_sets += mini_test_X\n",
    "\n",
    "\t\t#Finally add whole sets\n",
    "\t\ttraining_sets.append(self.x_train)\n",
    "\t\ttesting_sets.append(self.x_test)\n",
    "\t\tidx_len_tuples.append((0, self.current_params[\"sequence_length\"]))\n",
    "\n",
    "\t\tfor i, train_set in enumerate(training_sets):\n",
    "\t\t\td = idx_len_tuples[i]\n",
    "\t\t\tself.x_train = train_set\n",
    "\t\t\tself.x_test =  testing_sets[i]\n",
    "\n",
    "\t\t\t#Output size - in future delete any cols for categorical which are all zero\n",
    "\t\t\tmodel = generate_model(self.x_train, self.y_train, self.current_params)\n",
    "\n",
    "\t\t\th = model.fit(self.x_train, self.y_train,\n",
    "\t\t\t\tbatch_size=self.current_params[\"batch_size\"],\n",
    "\t\t\t\tepochs=self.current_params[\"epochs\"],\n",
    "\t\t\t\tverbose=0, shuffle=True, callbacks=[reset_states])\n",
    "\n",
    "\t\t\tself.metrics[\"train_acc_{}\".format(d)] = h.history[\"acc\"]\n",
    "\t\t\tpred_Y = model.predict(self.x_test, batch_size=self.current_params[\"batch_size\"])\n",
    "\t\t\tpreds.append(pred_Y)\n",
    "\t\t\tself.metrics[\"preds_{}\".format(d)] = [x[0] for x in pred_Y]\n",
    "\t\t\tself.metrics[\"acc_{}\".format(d)] = accuracy_score(self.y_test.flatten(), [np.round(x) for x in self.metrics[\"preds_{}\".format(d)]])\n",
    "\t\t\tprint(d, \"acc :\", self.metrics[\"acc_{}\".format(d)])\n",
    "\n",
    "\t\tself.write_up_models(models, preds)\n",
    "\t\treturn False\n",
    "\n",
    "\tdef run_experiments(self):\n",
    "\t\t# Only changeable parameter is sequence length\n",
    "\t\tself.num_experiments = len(self.sequence_lengths)\n",
    "\n",
    "\t\tfor s in self.sequence_lengths:\n",
    "\t\t\tself.run_one_experiment()\n",
    "\n",
    "\t\t# Experiments run - close data files\n",
    "\t\tprint(self.experiment_id, \" models run.\")\n",
    "\n",
    "\t\tself.metrics_file.close()\n",
    "\n",
    "\n",
    "class SlidingWindow(Experiment):\n",
    "\t\"\"\"Ensemble models for sub-seqeunces of data\"\"\"\n",
    "\tdef __init__(self, parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=None, y_test=None,\n",
    "\t\tx_train=None, y_train=None,\n",
    "\t\tdata=None, folds=10,\n",
    "\t\tfolder_name=str(time.time()),\n",
    "\t\tthresholding=False, threshold=0.5):\n",
    "\n",
    "\t\tsuper(SlidingWindow, self).__init__(parameters, search_algorithm=\"grid\", x_test=x_test, y_test=y_test, x_train=x_train, y_train=y_train, data=data, folds=folds, folder_name=folder_name, thresholding=thresholding, threshold=threshold)\n",
    "\n",
    "\n",
    "\tdef run_one_experiment(self):\n",
    "\t\tprint(\"run one experiment - sliding window\")\n",
    "\t\t#Get new configuration if random search\n",
    "\t\tif self.search_algorithm == \"random\":\n",
    "\t\t\tself.__random_config()\n",
    "\n",
    "\t\tself.experiment_id += 1\n",
    "\t\tprint(\"running expt\", self.experiment_id, \"of\", self.num_experiments)\n",
    "\t\tprint(self.current_params)\n",
    "\n",
    "\t\tself.metrics = {}\n",
    "\t\tself.current_fold = 1\n",
    "\t\tself.accuracy_scores = []\n",
    "\n",
    "\t\t# k-fold cross-validation\n",
    "\t\tif self.folds != None:\n",
    "\t\t\traise NotImplementedError(\"Sliding Window has not been implemented for k-fold validation\")\n",
    "\t\t# test-train\n",
    "\t\telse:\n",
    "\t\t\tself.x_train = deepcopy(self.X_TRAIN)\n",
    "\t\t\tself.y_train = deepcopy(self.Y_TRAIN)\n",
    "\t\t\tself.x_test = deepcopy(self.X_TEST)\n",
    "\t\t\tself.y_test = deepcopy(self.Y_TEST)\n",
    "\n",
    "\t\t\tprint(\"1---\", self.x_train.shape)\n",
    "\n",
    "\t\t\tself.test_idxs = np.array(range(1, len(self.y_test) + 1)) / 10 #divide by 10 to distinguish from training/10-fold data\n",
    "\n",
    "\t\t\tself.set_up_model()\n",
    "\n",
    "\tdef set_up_model(self):\n",
    "\n",
    "\t\tprint(\"---\", self.x_train.shape)\n",
    "\t\t# Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tmodels = []\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\t\tmetrics = self.metrics\n",
    "\t\tpreds = []\n",
    "\n",
    "\t\tall_x_train = []\n",
    "\t\tall_y_train = []\n",
    "\t\ttesting_x = []\n",
    "\t\ttesting_y = []\n",
    "\t\ttest_idxs = []\n",
    "\n",
    "\t\t#Get sliding window data\n",
    "\t\tseq_len = self.current_params[\"sequence_length\"]\n",
    "\t\tfor i in list(range(max([len(x) for x in self.x_train]) - seq_len)):\n",
    "\t\t\tfor x, y, store_x, store_y in zip(\n",
    "\t\t\t\t[self.x_train, self.x_test],\n",
    "\t\t\t\t[self.y_train, self.y_test],\n",
    "\t\t\t\t[all_x_train, testing_x],\n",
    "\t\t\t\t[all_y_train, testing_y],\n",
    "\t\t\t\t):\n",
    "\t\t\t\ttemp_x = []\n",
    "\t\t\t\ttemp_y = []\n",
    "\t\t\t\tidxs = []\n",
    "\n",
    "\t\t\t\tfor n in range(len(x)):\n",
    "\t\t\t\t\tif len(x[n][i:seq_len+i]) == seq_len:\n",
    "\t\t\t\t\t\tnew = np.array(x[n][i:seq_len+i])\n",
    "\t\t\t\t\t\tprint(new.shape, np.array([range(i, seq_len+i)]).T.shape)\n",
    "\t\t\t\t\t\ttemp_x.append(\n",
    "\t\t\t\t\t\t\tnp.concatenate((new, np.array([range(i, seq_len+i)]).T), axis=1)\n",
    "\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\ttemp_y.append(np.array(y[n]))\n",
    "\t\t\t\t\t\tif store_x is testing_x:\n",
    "\t\t\t\t\t\t\tidxs.append(self.test_idxs[n])\n",
    "\t\t\t\tif len(temp_y):\n",
    "\t\t\t\t\tstore_x.append(np.array(temp_x))\n",
    "\t\t\t\t\tstore_y.append(np.array(temp_y))\n",
    "\t\t\t\t\tif len(idxs):\n",
    "\t\t\t\t\t\ttest_idxs.append(idxs)\n",
    "\n",
    "\n",
    "\t\t# Concatenate training data\n",
    "\t\tself.x_train = np.concatenate(all_x_train)\n",
    "\t\tself.y_train = np.concatenate(all_y_train)\n",
    "\t\tself.x_test = testing_x\n",
    "\t\tself.y_test = testing_y\n",
    "\t\tself.test_idxs = test_idxs\n",
    "\n",
    "\t\t# scale data by test data mean and variance\n",
    "\t\tmeans, stdvs = get_mean_and_stdv(self.x_train)\n",
    "\t\tself.x_train = scale_array(self.x_train, means, stdvs)\n",
    "\t\tself.x_test = [scale_array(x, means, stdvs) for x in self.x_test]\n",
    "\n",
    "\n",
    "\t\tprint(2, self.x_train.shape, self.y_train.shape)\n",
    "\n",
    "\t\t# Shuffle trainig data\n",
    "\t\tself.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])\n",
    "\n",
    "\t\t#Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tprint(\"train, test set size (x):\", self.x_train.shape, [x.shape for x in self.x_test])\n",
    "\t\tmodel = generate_model(self.x_train, self.y_train, self.current_params, model_type=self.model_type)\n",
    "\n",
    "\t\t#if self.current_fold == 1:\n",
    "\t\t#\tprint(model.summary())\n",
    "\n",
    "\t\treturn self.train_model(model) #Returns TRUE if accuracy below threshold\n",
    "\n",
    "\n",
    "\tdef train_model(self, model):\n",
    "\t\t# Train model\n",
    "\t\tmodel = generate_model(self.x_train, self.y_train, self.current_params)\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\t\tstart_train = time.time()\n",
    "\t\th = model.fit(\n",
    "\t\t\tself.x_train, self.y_train,\n",
    "\t\t\tbatch_size=self.current_params[\"batch_size\"],\n",
    "\t\t\tepochs=self.current_params[\"epochs\"],\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tverbose=1,\n",
    "\t\t\tcallbacks=[reset_states])\n",
    "\t\tend_train = time.time()\n",
    "\n",
    "\t\t# test model\n",
    "\n",
    "\t\tfor i, test_set in enumerate(self.x_test):\n",
    "\t\t\tself.metrics[\"train_acc\"] = h.history[\"acc\"]\n",
    "\t\t\tself.metrics[\"slide_start\"] = i\n",
    "\n",
    "\t\t\tpred_Y = model.predict(test_set, batch_size=self.current_params[\"batch_size\"])\n",
    "\t\t\tself.metrics[\"preds\"] = [x[0] for x in pred_Y]\n",
    "\n",
    "\t\t\tmetrics = self.metrics\n",
    "\n",
    "\t\t\tmetrics[\"train_acc\"] = h.history[\"acc\"]\n",
    "\n",
    "\t\t\tmetrics[\"truth\"] = self.y_test[i].flatten().tolist()\n",
    "\t\t\tmetrics[\"categorical_preds\"] = [np.round(x) for x in metrics[\"preds\"]]\n",
    "\t\t\tmetrics[\"fscore\"] = f1_score(metrics[\"truth\"], metrics[\"categorical_preds\"])\n",
    "\t\t\tmetrics[\"accuracy\"] = accuracy_score(metrics[\"truth\"], metrics[\"categorical_preds\"])\n",
    "\t\t\tmetrics[\"experiment_id\"] = self.experiment_id\n",
    "\t\t\tmetrics[\"training_size\"] = len(self.y_train)\n",
    "\t\t\tmetrics[\"test_size\"] = len(self.y_test[i])\n",
    "\t\t\tmetrics[\"train_time\"] = end_train - start_train\n",
    "\t\t\tmetrics[\"test_idxs\"] = self.test_idxs\n",
    "\n",
    "\t\t\tif not self.metrics_headers:\n",
    "\t\t\t\t#Create files and Write file headers\n",
    "\t\t\t\tos.mkdir(self.folder_name)\n",
    "\t\t\t\tself.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())\n",
    "\t\t\t\tself.metrics_file = open(\"{}/results.csv\".format(self.folder_name), \"w\")\n",
    "\t\t\t\tself.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)\n",
    "\t\t\t\tself.metrics_writer.writeheader()\n",
    "\n",
    "\t\t\t#Write up metric results\n",
    "\t\t\tself.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))\n",
    "\n",
    "\t\t\tprint(\"acc:\", metrics[\"accuracy\"], \"fscore:\", metrics[\"fscore\"], \"start\", i, test_set.shape)\n",
    "\n",
    "\t\tdel model\n",
    "\t\tgc.collect()\n",
    "\n",
    "\t\treturn False\n",
    "\n",
    "class SlidingWindow_Support(SlidingWindow):\n",
    "\tdef __init__(self, parameters, search_algorithm=\"grid\",\n",
    "\t\tx_test=None, y_test=None,\n",
    "\t\tx_train=None, y_train=None,\n",
    "\t\tdata=None, folds=10,\n",
    "\t\tfolder_name=str(time.time()),\n",
    "\t\tthresholding=False, threshold=0.5):\n",
    "\n",
    "\t\tsuper(SlidingWindow, self).__init__(parameters, search_algorithm=\"grid\", x_test=x_test, y_test=y_test, x_train=x_train, y_train=y_train, data=data, folds=folds, folder_name=folder_name, thresholding=thresholding, threshold=threshold)\n",
    "\n",
    "\n",
    "\tdef set_up_model(self):\n",
    "\n",
    "\t\tprint(\"---\", self.x_train.shape)\n",
    "\t\t# Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tmodels = []\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\t\tmetrics = self.metrics\n",
    "\t\tpreds = []\n",
    "\n",
    "\t\ttesting_x = []\n",
    "\t\ttesting_y = []\n",
    "\t\ttest_idxs = []\n",
    "\n",
    "\t\t#Get sliding window data\n",
    "\t\tseq_len = self.current_params[\"sequence_length\"]\n",
    "\t\tfor i in list(range(max([len(x) for x in self.x_train]) - seq_len)):\n",
    "\t\t\tfor x, y, store_x, store_y in zip(\n",
    "\t\t\t\t[self.x_test],\n",
    "\t\t\t\t[self.y_test],\n",
    "\t\t\t\t[testing_x],\n",
    "\t\t\t\t[testing_y],\n",
    "\t\t\t\t):\n",
    "\t\t\t\ttemp_x = []\n",
    "\t\t\t\ttemp_y = []\n",
    "\t\t\t\tidxs = []\n",
    "\n",
    "\t\t\t\tfor n in range(len(x)):\n",
    "\t\t\t\t\tif len(x[n][i:seq_len+i]) == seq_len:\n",
    "\t\t\t\t\t\ttemp_x.append(np.array(x[n][i:seq_len+i]))\n",
    "\t\t\t\t\t\ttemp_y.append(np.array(y[n]))\n",
    "\t\t\t\t\t\tidxs.append(self.test_idxs[n])\n",
    "\t\t\t\tif len(temp_y):\n",
    "\t\t\t\t\tstore_x.append(np.array(temp_x))\n",
    "\t\t\t\t\tstore_y.append(np.array(temp_y))\n",
    "\t\t\t\t\ttest_idxs.append(idxs)\n",
    "\n",
    "\n",
    "\t\t# Concatenate training data\n",
    "\t\tself.x_train, self.y_train = remove_short(self.x_train, self.y_train, self.current_params[\"sequence_length\"])\n",
    "\t\tself.x_test = testing_x\n",
    "\t\tself.y_test = testing_y\n",
    "\t\tself.test_idxs = test_idxs\n",
    "\n",
    "\t\t# scale data by test data mean and variance\n",
    "\t\tmeans, stdvs = get_mean_and_stdv(self.x_train)\n",
    "\t\tself.x_train = scale_array(self.x_train, means, stdvs)\n",
    "\t\tself.x_test = [scale_array(x, means, stdvs) for x in self.x_test]\n",
    "\n",
    "\n",
    "\t\tprint(2, self.x_train.shape, self.y_train.shape)\n",
    "\n",
    "\t\t# Shuffle trainig data\n",
    "\t\tself.x_train, self.y_train = unison_shuffled_copies([self.x_train, self.y_train])\n",
    "\n",
    "\t\t#Output size - in future delete any cols for categorical which are all zero\n",
    "\t\tprint(\"train, test set size (x):\", self.x_train.shape, [x.shape for x in self.x_test])\n",
    "\t\tmodel = generate_model(self.x_train, self.y_train, self.current_params, model_type=self.model_type)\n",
    "\n",
    "\t\t#if self.current_fold == 1:\n",
    "\t\t#\tprint(model.summary())\n",
    "\n",
    "\t\treturn self.train_model(model) #Returns TRUE if accuracy below threshold\n",
    "\n",
    "class Omit_test_data(Experiment):\n",
    "\tdef train_model(self, model):\n",
    "\t\t\"\"\"run one fold and write up results\"\"\"\n",
    "\t\tprint(\"\t\tfold \", self.current_fold, \"of\", self.folds)\n",
    "\t\tmetrics = self.metrics\n",
    "\t\treset_states = ResetStatesCallback()\n",
    "\n",
    "\t\tif not len(self.y_test):\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tmodel.fit(\n",
    "\t\tself.x_train, self.y_train,\n",
    "\t\tbatch_size=self.current_params[\"batch_size\"],\n",
    "\t\tepochs=self.current_params[\"epochs\"],\n",
    "\t\tverbose=0,\n",
    "\t\tshuffle=True,\n",
    "\t\tcallbacks=[reset_states])\n",
    "\n",
    "\t\theaders = [x + \"_ON\" for x in self.headers[:len(self.x_test[0][0])]]\n",
    "\n",
    "\t\tindicies = list(range(len(headers)))\n",
    "\n",
    "\t\tfor num_missing in range(1, len(indicies)+1):\n",
    "\t\t\tfor subset in combinations(indicies, num_missing):\n",
    "\t\t\t\ttemp_test_X = deepcopy(self.x_test)\n",
    "\t\t\t\tstates = [1 for x in range(len(indicies))]\n",
    "\n",
    "\t\t\t\tfor feature_index in subset:\n",
    "\t\t\t\t\tstates[feature_index] = 0\n",
    "\t\t\t\t\ttemp_test_X[:,:,feature_index] = 0 # because 0 is the mean of the training data\n",
    "\t\t\t\tprint(states)\n",
    "\n",
    "\t\t\t\tpred_Y = model.predict(temp_test_X, batch_size=self.current_params[\"batch_size\"]) #Predicted value of Y\n",
    "\t\t\t\tpred_Y = [x[0] for x in pred_Y]\n",
    "\t\t\t\tY_classes =  [np.round(x) for x in pred_Y] #Choose a class\n",
    "\t\t\t\tmetrics[\"preds\"] = pred_Y\n",
    "\t\t\t\tmetrics[\"categorical_preds\"] = Y_classes\n",
    "\t\t\t\tmetrics[\"truths\"] = [int(x[0]) for x in self.y_test]\n",
    "\t\t\t\tmetrics[\"variance\"] = np.var(pred_Y)\n",
    "\t\t\t\tmetrics[\"accuracy\"] = accuracy_score(self.metrics[\"truths\"], self.metrics[\"categorical_preds\"])\n",
    "\t\t\t\tmetrics[\"fmeasure\"] = accuracy_score(self.metrics[\"truths\"], self.metrics[\"categorical_preds\"])\n",
    "\t\t\t\tfor h in range(len(headers)):\n",
    "\t\t\t\t\tmetrics[headers[h]] = states[h]\n",
    "\n",
    "\t\t\t\tif not self.metrics_headers:\n",
    "\t\t\t\t\t#Create files and Write file headers\n",
    "\t\t\t\t\tos.mkdir(self.folder_name)\n",
    "\t\t\t\t\tself.metrics_headers = list(metrics.keys()) + list(self.current_params.keys())\n",
    "\t\t\t\t\tself.metrics_file = open(\"{}/results.csv\".format(self.folder_name), \"w\")\n",
    "\t\t\t\t\tself.metrics_writer = csv.DictWriter(self.metrics_file, fieldnames=self.metrics_headers)\n",
    "\t\t\t\t\tself.metrics_writer.writeheader()\n",
    "\n",
    "\t\t\t\t#Write up metric results\n",
    "\t\t\t\tself.metrics_writer.writerow(merge_two_dicts(self.current_params, self.metrics))\n",
    "\n",
    "\t\t\t\t#Search type changes\n",
    "\t\t\t\tprint(\"acc:\", metrics[\"accuracy\"], \"features on:\", str(states))\n",
    "\n",
    "\t\t#make space in memory\n",
    "\t\tdel model\n",
    "\t\tgc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

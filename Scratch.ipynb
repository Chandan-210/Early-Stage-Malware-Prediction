{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799e7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras import callbacks\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(arrays):\n",
    "    \"\"\"shuffle multiple arrays keeping\n",
    "    corresponding items in same relative positions\n",
    "    between arrays\"\"\"\n",
    "    # print(len(arrays))\n",
    "    # print(len(arrays[0]))\n",
    "    length = len(arrays[1])\n",
    "    # print(length)\n",
    "\n",
    "    for x in arrays:\n",
    "        print(x)\n",
    "        print(len(x))\n",
    "        assert len(x) == length\n",
    "    p = np.random.permutation(length)\n",
    "    return tuple([x[p] for x in arrays])\n",
    "\n",
    "\n",
    "def to_numpy_tensors(x, y):\n",
    "    \"\"\" return x and y as numpy tensors,\n",
    "    x as a 3-D tensor, y as a 2-D tensor\"\"\"\n",
    "\n",
    "    #x_shape = (len(x), len(x[0]), len(x[0][0]))\n",
    "    #y_shape = (len(y), 1)\n",
    "\n",
    "    if type(x) is list:\n",
    "        x = np.array(x)\n",
    "    if type(y) is list:\n",
    "        y = np.array(y)\n",
    "\n",
    "    #x = x.reshape(x_shape)\n",
    "    #y = y.reshape(y_shape)\n",
    "    x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_mean_and_stdv(dataset):\n",
    "    \"\"\"return means and standard\n",
    "    deviations along 0th axis of tensor\"\"\"\n",
    "    means = dataset.mean(0)\n",
    "    stdvs = dataset.std(0)\n",
    "    return means, stdvs\n",
    "\n",
    "\n",
    "def scale_array(dataset, means, stdvs):\n",
    "    \"\"\"normalise by means and standard\n",
    "    deviations along 0th axis of tensor\"\"\"\n",
    "    return (dataset - means) / (stdvs + K.epsilon())\n",
    "\n",
    "\n",
    "def check_filename(originalName):\n",
    "    \"\"\" add number to filename to make\n",
    "    unique if not already - returns new name\"\"\"\n",
    "    count = 1\n",
    "    exists = True\n",
    "    fileName = originalName\n",
    "    while(exists):\n",
    "        exists = os.path.exists(fileName)\n",
    "        if exists:\n",
    "            if \".\" in originalName:\n",
    "                fileName = originalName.split(\".\")\n",
    "                fileName[0] += \"_\" + str(count)\n",
    "                fileName = \".\".join(fileName)\n",
    "            else:\n",
    "                fileName = originalName + \"_\" + str(count)\n",
    "            count += 1\n",
    "    return fileName\n",
    "\n",
    "\n",
    "def truncate_and_tensor(dataX, dataY, length):\n",
    "    \"\"\"crop any longer than length,\n",
    "    return new X and Y tensors\"\"\"\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "\n",
    "    for x in list(range(len(dataX))):\n",
    "        new_X.append(dataX[x][:length])\n",
    "        new_Y.append(dataY[x][:length])\n",
    "\n",
    "    x, y = to_numpy_tensors(new_X, new_Y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def remove_short(dataX, dataY, length):\n",
    "    \"\"\"Remove any seqences in dataX shorter than length,\n",
    "    crop any longer than length, return new X and Y tensors\"\"\"\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    '''\n",
    "    for x in list(range(len(dataX))):\n",
    "        if len(dataX[x]) >= length:\n",
    "            new_X.append(dataX[x][:length])\n",
    "            new_Y.append(dataY[x][:length])\n",
    "\n",
    "    x, y = to_numpy_tensors(new_X, new_Y)\n",
    "    '''\n",
    "    x, y = to_numpy_tensors(dataX, dataY)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def remove_short_idx(dataX, dataY, idxs, length):\n",
    "    \"\"\" Remove any seqences in dataX shorter than length,\n",
    "    crop any longer than length, return new X and Y tensors\n",
    "    and indicies of the returned samples with refernce to\n",
    "    their position in the original array\"\"\"\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    #new_idxs = []\n",
    "    # dataX[x] will kinda wil always be greater than length\n",
    "    '''\n",
    "    for x in list(range(len(dataX))):\n",
    "        if len(dataX[x]) >= length:\n",
    "            new_X.append(dataX[x][:length])\n",
    "            new_Y.append(dataY[x][:length])\n",
    "            new_idxs.append(idxs[x])\n",
    "\n",
    "    x, y = to_numpy_tensors(new_X, new_Y)\n",
    "    '''\n",
    "    # new_idxs.append(idxs)\n",
    "    x, y = to_numpy_tensors(dataX, dataY)\n",
    "    # print(new_idxs)\n",
    "    # print(len(new_idxs))\n",
    "    # print(np.array(new_idxs))\n",
    "    # print(len(np.array(new_idxs[0])))\n",
    "    # return x, y, np.array(new_idxs)\n",
    "    return x, y, idxs\n",
    "\n",
    "\n",
    "def timestamped_to_vector(data, classification_col=1):\n",
    "    \"\"\"Return tuple of inputs and outputs)\n",
    "    Removes timestamp - not desirable if irregular inputs\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    temp_inputs = []\n",
    "    temp_outputs = []\n",
    "    #malware = []\n",
    "    #benign = []\n",
    "    #malware_value = []\n",
    "    #benign_value = []\n",
    "    for input_row in data:\n",
    "        keep = np.array([x for x in range(len(input_row)) if x not in [classification_col]])\n",
    "        # print(keep)\n",
    "\n",
    "        x.append(input_row[keep])  # Advanced indexing. Only select/keep elements at indexes 0 to 54. columns(0 - 54). 55 is left out since it is malware_col\n",
    "        assert int(input_row[classification_col]) in [0, 1]\n",
    "        y.append([int(input_row[classification_col])])\n",
    "\n",
    "       # assert int(input_row[classification_col]) in [0, 1]\n",
    "        # if int(input_row[classification_col]) == 0:\n",
    "        #    benign.append(input_row[keep])\n",
    "        # if int(input_row[classification_col]) not in benign_value:\n",
    "        #    benign_value.append(0)\n",
    "    # else:\n",
    "    #    malware.append(input_row[keep])\n",
    "    #    if int(input_row[classification_col]) not in malware_value:\n",
    "    #        malware_value.append(1)\n",
    "    '''\n",
    "    if (temp_inputs != [] and temp_outputs != []):\n",
    "        x.append(temp_inputs)\n",
    "        y.append(temp_outputs)\n",
    "        temp_inputs = []\n",
    "        temp_outputs = []\n",
    "    '''\n",
    "    # if benign != [] and malware != []:\n",
    "    #   x.append(benign)\n",
    "    #  x.append(malware)\n",
    "    # y.append(benign_value)\n",
    "    # y.append(malware_value)\n",
    "\n",
    "    x = np.asarray(x)  # convert existing python sequence (list, list of tuples etc) into ndarray\n",
    "    y = np.asarray(y)\n",
    "    return x, y  # return tuple\n",
    "\n",
    "\n",
    "def array_to_list(arr):\n",
    "    try:\n",
    "        return arr.tolist()\n",
    "    except AttributeError:\n",
    "        return arr\n",
    "\n",
    "\n",
    "def into_sliding_chunk_arrays(data_x, chunk_size):\n",
    "    \"\"\"return array of data cut into chunks of size chunk_size sliding along array\n",
    "    and array of tuples denoting starting index and end index for chunk\"\"\"\n",
    "    big_X = []\n",
    "    data_temp = array_to_list(data_x)\n",
    "    idx_len_tuples = list(range(len(data_temp[0]) - chunk_size + 1)) if ids == [] else ids\n",
    "\n",
    "    for i in list(range(len(data_temp[0]) - chunk_size + 1)):\n",
    "        x = []\n",
    "        for s, sequence in enumerate(data_temp):\n",
    "            x.append(sequence[i:i + chunk_size])\n",
    "        big_X.append(np.array(x))\n",
    "\n",
    "    return big_X, idx_len_tuples\n",
    "\n",
    "\n",
    "def sliding_window_data(data_x, chunk_size):\n",
    "    \"\"\"return array of data cut into chunks of size chunk_size sliding along array\n",
    "    and array of tuples denoting starting index and end index for chunk\"\"\"\n",
    "    big_X = []\n",
    "    data_temp = array_to_list(data_x)\n",
    "    idx_len_tuples = list(range(len(data_temp[0]) - chunk_size + 1)) if ids == [] else ids\n",
    "\n",
    "    for i in list(range(len(data_temp[0]) - chunk_size + 1)):\n",
    "        x = []\n",
    "        for s, sequence in enumerate(data_temp):\n",
    "            x.append(sequence[i:i + chunk_size])\n",
    "        big_X.append(np.array(x))\n",
    "\n",
    "    return big_X, idx_len_tuples\n",
    "\n",
    "\n",
    "def to_chunks(l, num_chunks):\n",
    "    last_one = 0\n",
    "    chunks = []\n",
    "    n = len(l) // num_chunks\n",
    "\n",
    "    # Most samples spread out\n",
    "    for i in range(0, n * num_chunks, n):\n",
    "        chunks.append(l[i:i + n])\n",
    "    # Extras distributed evenly:\n",
    "    for i in range(n * num_chunks, len(l)):\n",
    "        chunks[last_one].append(l[i])\n",
    "        last_one = (last_one + 1) % num_chunks\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Given two dicts, merge them into a new dict as a shallow copy. Credit: http://stackoverflow.com/questions/38987/how-to-merge-two-python-dictionaries-in-a-single-expression\"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "class ResetStatesCallback(callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.model.reset_states()\n",
    "\n",
    "\n",
    "def extract_val_set_binary(input_data, targets, val_percentage):\n",
    "    change_every = 1 / val_percentage  # add to val indicies every 1/percentage\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    classes = [pos, neg]\n",
    "    train_long_tensor = []\n",
    "    val_long_tensor = []\n",
    "    for t, target in enumerate(targets):\n",
    "        int_targ = int(target[0])\n",
    "        classes[int_targ] += 1\n",
    "        if classes[int_targ] % change_every == 0:\n",
    "            val_long_tensor.append(t)\n",
    "        else:\n",
    "            train_long_tensor.append(t)\n",
    "\n",
    "    # return reduced training set and validation set\n",
    "    train_data = input_data[train_long_tensor]\n",
    "    train_targets = targets[train_long_tensor]\n",
    "    val_data = input_data[val_long_tensor]\n",
    "    val_targets = targets[val_long_tensor]\n",
    "    return train_data, train_targets, val_data, val_targets\n",
    "\n",
    "\n",
    "def extract_neg(input_data, targets, val_percentage):\n",
    "    change_every = 1 / val_percentage  # add to val indicies every 1/percentage\n",
    "    pos = []\n",
    "    neg = []\n",
    "    classes = [pos, neg]\n",
    "    train_long_tensor = []\n",
    "    val_long_tensor = []\n",
    "    for t, target in enumerate(targets):\n",
    "        int_targ = int(target[0])\n",
    "        classes[int_targ].append(t)\n",
    "\n",
    "    pos = pos[-len(neg):]\n",
    "    train_long_tensor = pos + neg\n",
    "\n",
    "    # return reduced training set and validation set\n",
    "    train_data = input_data[train_long_tensor]\n",
    "    train_targets = targets[train_long_tensor]\n",
    "    return train_data, train_targets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
